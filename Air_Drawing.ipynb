{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Imports\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import kagglehub\n",
    "import keras\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.applications import MobileNetV2, ResNet50V2\n",
    "from keras import layers, models\n",
    "from keras.optimizers import Adam\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, LeakyReLU, BatchNormalization\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Handtracking and Image modification\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "from skimage.morphology import skeletonize\n",
    "from skimage.measure import label, regionprops"
   ],
   "id": "72515e11f43eee7b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Projet - Air Drawing\n",
    "---\n",
    "EXPLICATION DE LA PROBLEMATIQUE"
   ],
   "id": "f9157f417d992e1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Datsets\n",
    "---"
   ],
   "id": "31761a2103224f69"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Autre Dataset\n",
    "Explication de pourquoi est ce qu'on ne la pas pris car moins bien etc\n",
    "Solution : EMNIST"
   ],
   "id": "86db007aa8643fed"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### EMNIST\n",
    "Le jeu de données EMNIST (Extended MNIST) est un ensemble de caractères manuscrits dérivé de la base de données NIST Special Database 19. Il a été converti au format d’image 28x28 pixels, avec une structure de données qui correspond directement à celle du jeu de données MNIST. EMNIST étend MNIST en incluant non seulement des chiffres, mais aussi des lettres majuscules et minuscules manuscrites, offrant ainsi un ensemble plus riche pour les tâches de reconnaissance de caractères."
   ],
   "id": "6afbf0e500f774a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Télécharger dans le dossier par défaut non modifiable (cache de kagglehub)\n",
    "dataset_path = kagglehub.dataset_download(\"crawford/emnist\")\n",
    "\n",
    "# Dossier cible\n",
    "custom_path = \"./Datasets/emnist_datasets\"\n",
    "os.makedirs(custom_path, exist_ok=True)\n",
    "\n",
    "# Parcourir tout ce qu’il y a dans dataset_path\n",
    "for item in os.listdir(dataset_path):\n",
    "    src = os.path.join(dataset_path, item)\n",
    "    dst = os.path.join(custom_path, item)\n",
    "    shutil.move(src, dst)"
   ],
   "id": "a2dffe985c21bdaf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### EMNIST (By_Class)\n",
    "L'ensemble complet de la base de données EMNIST est disponible dans le sous-ensembles ByClass.\n",
    "Le jeu ByClass contient 62 classes distinctes : les 10 chiffres (0–9), les 26 lettres majuscules (A–Z) et les 26 lettres minuscules (a–z).\n",
    "La répartition est déséquilibrée : certaines classes ont beaucoup plus d’exemples que d’autres.\n",
    "La fréquence des lettres reflète à peu près leur fréquence d'usage dans la langue anglaise.\n",
    "\n",
    "__Taille des ensembles :__\n",
    "- Entraînement : 697 932 images\n",
    "- Test : 116 323 images\n",
    "- Total : 814 255 images\n",
    "\n",
    "__Classes :__\n",
    "- ByClass : 62 classes (déséquilibrées)\n"
   ],
   "id": "76e9da62ae9c2fbd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Recupération des train_data de EMNITST\n",
    "train_data = pd.read_csv('./Datasets/emnist_datasets/emnist-byclass-train.csv', header=None, nrows=300000).to_numpy()\n",
    "# Dataset Comprends 697 932 entrées, par raison de performances nous avons limités la charge à 300K.\n",
    "print(train_data.shape)"
   ],
   "id": "348b9dcd148af826"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Recupération des test_data de EMNITST\n",
    "test_data = pd.read_csv('./Datasets/emnist_datasets/emnist-byclass-test.csv', header=None).to_numpy()\n",
    "print(test_data.shape)"
   ],
   "id": "2270763d71856610"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Les test_data permet au modèle de voir s'il se généralise bien et qu'il ne fait pas du sur-apprentissage (évaluation époque par époque).\n",
    "Pratique pour stopper ou débugger l'entrainement du modèle."
   ],
   "id": "138dee2d6c11a222"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Recupération des label_mapping de EMNITST\n",
    "label_mapping = np.genfromtxt('./Datasets/emnist_datasets/emnist-byclass-mapping.txt', delimiter=' ')\n",
    "\n",
    "label_trans = {}\n",
    "for label in label_mapping:\n",
    "    label_trans[label[0]] = chr(int(label[1]))\n",
    "\n",
    "# Exemple d'une donnée EMNIST\n",
    "img_nb = 150\n",
    "print(label_trans[train_data[img_nb,0]])\n",
    "plt.imshow(train_data[img_nb,1:].reshape(28,28).T, cmap='gray')\n",
    "plt.show()"
   ],
   "id": "530c68c004667957"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Préparation des données :\n",
    "\n",
    "# 1) Séparation des labels \"y\" et les données de l'image \"x\"\n",
    "# Entrainement\n",
    "train_x = train_data[:,1:]\n",
    "train_y = train_data[:,0]\n",
    "# Test\n",
    "test_x = test_data[:,1:]\n",
    "test_y = test_data[:,0]\n",
    "\n",
    "# 2) Normalisation (valeur des \"x\" entre 0 et 1) -> Plus facile pour l'entrainement\n",
    "train_x = train_x / 255.0\n",
    "test_x = test_x / 255.0\n",
    "\n",
    "# 3) Reshape (Keras attend un format [batch_size, hauteur, largeur, canaux])\n",
    "train_count = train_x.shape[0]\n",
    "train_x = train_x.reshape(train_count, 28, 28, 1)\n",
    "\n",
    "test_count = test_x.shape[0]\n",
    "test_x = test_x.reshape(test_count, 28, 28, 1)\n",
    "\n",
    "# 4) Transformation des labels (Chiffre en vecteur binaire)\n",
    "num_classes = 63 # (62 catégorie allant de 1 à 62 donc liste de longueur 63)\n",
    "\n",
    "train_y = keras.utils.to_categorical(train_y, num_classes)\n",
    "test_y = keras.utils.to_categorical(test_y, num_classes)"
   ],
   "id": "52c086e324ec6cc9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Nos données provenant du dataset EMNIST(By_Class) sont maintanant pretes à etre utiliser dans nos différents modèle.",
   "id": "3650e8801ef12f35"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### EMNIST (Letters)\n",
    "Le jeu de données EMNIST Letters fusionne un ensemble équilibré de lettres majuscules et minuscules en une seule tâche de classification à 26 classes (une par lettre de l’alphabet).\n",
    "\n",
    "- Entraînement : 88 800 images\n",
    "- Test : 14 800 images\n",
    "- Total : 103 600 images\n",
    "- Nombre de classes : 26 (répartition équilibrée)\n"
   ],
   "id": "3d2a5349da6d575b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Entrainement de lettres\n",
    "---"
   ],
   "id": "fc153857886dc1a6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Option A (CNN Maison)",
   "id": "3f895474dc7222da"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### CCN Maison 1",
   "id": "c38af1e1c2488e61"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Modèle",
   "id": "e68773fc9fa5d38f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Définition du premier modèle\n",
    "model_cnn1 = Sequential()\n",
    "\n",
    "# Creating conv layer 1\n",
    "model_cnn1.add(Conv2D(32, kernel_size=(3, 3), activation='linear', padding='same', input_shape=[28, 28, 1]))\n",
    "model_cnn1.add(LeakyReLU(alpha=0.1))\n",
    "model_cnn1.add(MaxPooling2D((2, 2), padding='same'))\n",
    "model_cnn1.add(Dropout(0.25))\n",
    "\n",
    "# Creating conv layer 2\n",
    "model_cnn1.add(Conv2D(64, (3, 3), activation='linear', padding='same'))\n",
    "model_cnn1.add(LeakyReLU(alpha=0.1))\n",
    "model_cnn1.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "model_cnn1.add(Dropout(0.25))\n",
    "\n",
    "# Adding the dense final part\n",
    "model_cnn1.add(Flatten())\n",
    "model_cnn1.add(Dense(1024, activation='linear'))\n",
    "model_cnn1.add(LeakyReLU(alpha=0.1))\n",
    "model_cnn1.add(Dropout(0.25))\n",
    "model_cnn1.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model_cnn1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model_cnn1.summary()"
   ],
   "id": "b731265ac3cd6d24"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Apprentissage du modèle\n",
    "model_cnn1_history = model_cnn1.fit(train_x, train_y, validation_data=(test_x, test_y), epochs=10)"
   ],
   "id": "2df8c0d3313ca6af"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Tableau d'apprentissage",
   "id": "eaf62a818eec60c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Valeurs importantes lors de l'apprentissage du modèle\n",
    "model_train_acc_cnn1 = model_cnn1_history.history['accuracy']\n",
    "model_valid_acc_cnn1 = model_cnn1_history.history['val_accuracy']\n",
    "model_train_loss_cnn1 = model_cnn1_history.history['loss']\n",
    "model_valid_loss_cnn1 = model_cnn1_history.history['val_loss']\n",
    "\n",
    "# Graphiques de l'apprentissage\n",
    "fig,(ax0,ax1) = plt.subplots(1, 2, figsize=(15,4))\n",
    "\n",
    "# Accuracy graph\n",
    "ax0.plot(model_train_acc_cnn1, label=\"Train Acc.\")\n",
    "ax0.plot(model_valid_acc_cnn1, label=\"Valid. Acc.\")\n",
    "\n",
    "ax0.set_xlabel('Epoch')\n",
    "ax0.set_ylabel('Accuracy(%)')\n",
    "ax0.legend(loc='lower right', fancybox=True, shadow=True, ncol=4)\n",
    "ax0.grid()\n",
    "\n",
    "# Loss graph\n",
    "ax1.plot(model_train_loss_cnn1, label=\"Train Loss\")\n",
    "ax1.plot(model_valid_loss_cnn1, label=\"Valid. Loss\")\n",
    "\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend(loc='upper right', fancybox=True, shadow=True, ncol=4)\n",
    "ax1.grid()"
   ],
   "id": "9442dfdb15ebdad6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### CCN Maison 2",
   "id": "2163d2bd3e6a00ab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Modèle",
   "id": "c4ec8c8062e0b350"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Définition du deuxième modèle\n",
    "model_cnn2 = Sequential()\n",
    "\n",
    "# Convolution 1\n",
    "model_cnn2.add(Conv2D(32, kernel_size=(3, 3), padding='same', input_shape=(28, 28, 1)))\n",
    "model_cnn2.add(BatchNormalization())\n",
    "model_cnn2.add(LeakyReLU(alpha=0.1))\n",
    "model_cnn2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_cnn2.add(Dropout(0.2))\n",
    "\n",
    "# Convolution 2\n",
    "model_cnn2.add(Conv2D(64, kernel_size=(3, 3), padding='same'))\n",
    "model_cnn2.add(BatchNormalization())\n",
    "model_cnn2.add(LeakyReLU(alpha=0.1))\n",
    "model_cnn2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_cnn2.add(Dropout(0.3))\n",
    "\n",
    "# Convolution 3\n",
    "model_cnn2.add(Conv2D(128, kernel_size=(3, 3), padding='same'))\n",
    "model_cnn2.add(BatchNormalization())\n",
    "model_cnn2.add(LeakyReLU(alpha=0.1))\n",
    "model_cnn2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_cnn2.add(Dropout(0.4))\n",
    "\n",
    "# Dense layers\n",
    "model_cnn2.add(Flatten())\n",
    "model_cnn2.add(Dense(256))\n",
    "model_cnn2.add(LeakyReLU(alpha=0.1))\n",
    "model_cnn2.add(Dropout(0.5))\n",
    "\n",
    "model_cnn2.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compilation\n",
    "model_cnn2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Résumé\n",
    "model_cnn2.summary()"
   ],
   "id": "65fc43b4c9428ec4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Apprentissage du modèle\n",
    "model_cnn2_history = model_cnn2.fit(train_x, train_y, validation_data=(test_x, test_y), epochs=10)"
   ],
   "id": "72d8ce8f1f6ffbda"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Tableau d'apprentissage",
   "id": "9259c40f666cc04c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Valeurs importantes lors de l'apprentissage du modèle\n",
    "model_train_acc_cnn2 = model_cnn2_history.history['accuracy']\n",
    "model_valid_acc_cnn2 = model_cnn2_history.history['val_accuracy']\n",
    "model_train_loss_cnn2 = model_cnn2_history.history['loss']\n",
    "model_valid_loss_cnn2 = model_cnn2_history.history['val_loss']\n",
    "\n",
    "# Graphiques de l'apprentissage\n",
    "fig,(ax0,ax1) = plt.subplots(1, 2, figsize=(15,4))\n",
    "\n",
    "# Accuracy graph\n",
    "ax0.plot(model_train_acc_cnn2, label=\"Train Acc.\")\n",
    "ax0.plot(model_valid_acc_cnn2, label=\"Valid. Acc.\")\n",
    "\n",
    "ax0.set_xlabel('Epoch')\n",
    "ax0.set_ylabel('Accuracy(%)')\n",
    "ax0.legend(loc='lower right', fancybox=True, shadow=True, ncol=4)\n",
    "ax0.grid()\n",
    "\n",
    "# Loss graph\n",
    "ax1.plot(model_train_loss_cnn2, label=\"Train Loss\")\n",
    "ax1.plot(model_valid_loss_cnn2, label=\"Valid. Loss\")\n",
    "\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend(loc='upper right', fancybox=True, shadow=True, ncol=4)\n",
    "ax1.grid()"
   ],
   "id": "b32ad49c89a61204"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Option B (Transfert Learning)",
   "id": "a7d0def1c5b1da9e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Transfer Learning 1 (MobileNetV2)",
   "id": "d3a0f8ffffa5125a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Modèle Feature Extraction",
   "id": "584231cdef26ca76"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# We will use MobileNetV2 as the base model with pre-trained weights\n",
    "model_TL1_Base = MobileNetV2(weights=\"imagenet\", include_top=False, )  # Adjust input shape\n",
    "\n",
    "# Freeze the base model so its weights are not updated during training\n",
    "model_TL1_BASE.trainable = False\n",
    "\n",
    "model_TL1_FE = models.Sequential([\n",
    "    # Convert grayscale images (28x28x1) to 3-channel (RGB) images\n",
    "    layers.Lambda(lambda x: tf.image.grayscale_to_rgb(x)),\n",
    "\n",
    "    # Use MobileNetV2 as the feature extractor (excluding the top layer)\n",
    "    base_model,\n",
    "\n",
    "    # Add global average pooling layer\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "\n",
    "    # Add dense layer for classification\n",
    "    layers.Dense(128, activation='relu'),\n",
    "\n",
    "    # Add dropout for regularization\n",
    "    layers.Dropout(0.3),\n",
    "\n",
    "    # Output layer with 62 classes (for EMNIST Letters)\n",
    "    layers.Dense(62, activation='softmax')  # 62 classes for letters (A-Z) and digits (0-9)\n",
    "])\n",
    "\n",
    "model_CNN1.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                   loss='categorical_crossentropy',  # Use categorical crossentropy for multi-class classification\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "model_CNN1.fit(model_CNN1, validation_data=test_ds, epochs=10)\n",
    "print(base_model.summary())\n"
   ],
   "id": "b478c993a01d5535"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Tableau d'apprentissage (Feature Extraction)",
   "id": "c71462205adda516"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# TO DO",
   "id": "96d59f4adf8f670e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Modèle Fine-tuning",
   "id": "774993a69a3fdb72"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1. Unfreeze top layers of the base model\n",
    "base_model = model_CNN1.layers[0]  # Access MobileNetV2 inside the Sequential\n",
    "base_model.trainable = True\n",
    "\n",
    "# 2. Freeze most layers, keep top N trainable\n",
    "for layer in base_model.layers[:-30]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# 3. Recompile with a very low learning rate\n",
    "from keras.optimizers import Adam\n",
    "model_TL1_FT.compile(optimizer=Adam(learning_rate=1e-5),\n",
    "                   loss='sparse_categorical_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "# 4. Continue training\n",
    "fine_tune_epochs = 3\n",
    "model_CNN1.fit(train_ds, validation_data=test_ds, epochs=fine_tune_epochs)\n"
   ],
   "id": "a5a00ff3e5156740"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Tableau d'apprentissage (Fine-tuning)",
   "id": "a9bd63409aa4d94a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# TO DO",
   "id": "9bd3b2d82eac2bcd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Transfer Learning 2 (ResNet50V2)",
   "id": "18761666e740b407"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Modèle Feature Extraction",
   "id": "e10dc9e2afd8351f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load EMNIST Letters dataset\n",
    "def load_emnist_letters(root=\"./data\", train=True):\n",
    "    return torchvision.datasets.EMNIST(\n",
    "        root=root,\n",
    "        split=\"letters\",\n",
    "        train=train,\n",
    "        download=False,\n",
    "        transform=torchvision.transforms.ToTensor()\n",
    "    )"
   ],
   "id": "7468a61cea961543"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Convert EMNIST to numpy arrays\n",
    "def emnist_to_numpy(dataset, max_samples=None):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for i, (img, label) in enumerate(dataset):\n",
    "        if max_samples and i >= max_samples:\n",
    "            break\n",
    "        img_np = img.numpy().squeeze()\n",
    "        images.append(img_np)\n",
    "        labels.append(label - 1)  # EMNIST Letters labels start at 1\n",
    "    return np.array(images), np.array(labels)"
   ],
   "id": "9955b5e5b396b2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Build the transfer learning model\n",
    "def build_transfer_model():\n",
    "    base_model = ResNet50V2(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False  # Freeze base model for feature extraction\n",
    "\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ],
   "id": "d8bfc65d92fd972d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def build_transfer_model():\n",
    "    base_model = ResNet50V2(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
    "    base_model.trainable = False\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(26, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ],
   "id": "c446b565909a12d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Batch the data\n",
    "def create_emnist_tf_dataset(images, labels, batch_size=32, shuffle=True):\n",
    "    def preprocess(x, y):\n",
    "        x = tf.stack([x, x, x], axis=-1)\n",
    "        x = tf.image.resize(x, [224, 224])\n",
    "        return x, y\n",
    "    ds = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(images))\n",
    "    ds = ds.map(preprocess).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds"
   ],
   "id": "57d938cc93a529d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_dataset = load_emnist_letters(train=True)\n",
    "test_dataset = load_emnist_letters(train=False)\n",
    "train_images, train_labels = emnist_to_numpy(train_dataset, max_samples=10000)\n",
    "test_images, test_labels = emnist_to_numpy(test_dataset, max_samples=2000)\n",
    "train_ds = create_emnist_tf_dataset(train_images, train_labels)\n",
    "test_ds = create_emnist_tf_dataset(test_images, test_labels, shuffle=False)\n",
    "\n",
    "# Train the model\n",
    "model_CNN1 = build_transfer_model()\n",
    "history = model_CNN1.fit(train_ds, validation_data=test_ds, epochs=5)"
   ],
   "id": "e9ee4f69594aebf2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Tableau d'apprentissage (Feature Extraction)",
   "id": "7c2a6eeac118b0a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1. Unfreeze top layers of the base model\n",
    "base_model = model_CNN1.layers[0]  # Access MobileNetV2 inside the Sequential\n",
    "base_model.trainable = True\n",
    "\n",
    "# 2. Freeze most layers, keep top N trainable\n",
    "for layer in base_model.layers[:-30]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# 3. Recompile with a very low learning rate\n",
    "from keras.optimizers import Adam\n",
    "model_CNN1.compile(optimizer=Adam(learning_rate=1e-5),\n",
    "                   loss='sparse_categorical_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "# 4. Continue training\n",
    "fine_tune_epochs = 3\n",
    "model_CNN1.fit(train_ds, validation_data=test_ds, epochs=fine_tune_epochs)"
   ],
   "id": "743c38af868e95fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 0. Unfreeze top layers of the base model\n",
    "base_model = model_CNN1.layers[-1]  # Access MobileNetV2 inside the Sequential\n",
    "base_model.trainable = True\n",
    "\n",
    "# 1. Freeze most layers, keep top N trainable\n",
    "for layer in base_model.layers[:-31]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# 2. Recompile with a very low learning rate\n",
    "from keras.optimizers import Adam\n",
    "model_CNN1.compile(optimizer=Adam(learning_rate=0e-5),\n",
    "                   loss='sparse_categorical_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "# 3. Continue training\n",
    "fine_tune_epochs = 2\n",
    "model_CNN1.fit(train_ds, validation_data=test_ds, epochs=fine_tune_epochs)"
   ],
   "id": "3ee7835e4ee1a8b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "db3c1906902e231a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Modèle Fine-tuning",
   "id": "ca9ed0dcc0b8eab0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1. Unfreeze top layers of the base model\n",
    "base_model = model_CNN1.layers[0]  # Access MobileNetV2 inside the Sequential\n",
    "base_model.trainable = True\n",
    "\n",
    "# 2. Freeze most layers, keep top N trainable\n",
    "for layer in base_model.layers[:-30]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# 3. Recompile with a very low learning rate\n",
    "from keras.optimizers import Adam\n",
    "model_CNN1.compile(optimizer=Adam(learning_rate=1e-5),\n",
    "                   loss='sparse_categorical_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "# 4. Continue training\n",
    "fine_tune_epochs = 3\n",
    "model_CNN1.fit(train_ds, validation_data=test_ds, epochs=fine_tune_epochs)"
   ],
   "id": "874bb0fc1e2f9ff1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Tableau d'apprentissage (Fine-tuning)",
   "id": "62d4d51ade6d7cb0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# TO DO",
   "id": "3ee9b3380707da6e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Évaluation Comparative et Analyse Critique\n",
    "---\n",
    "Explication et résumer des résultat avec les tableaux etc\n",
    "Meilleur modele dans quel cas et pourquoi (temps, MSE, accuracy, etc)"
   ],
   "id": "de71701f394b834d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Application réel du projet\n",
    "---"
   ],
   "id": "64fb4c870213c8c1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Vidéo\n",
   "id": "c4ea0454f53800a9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === Fonctions utilitaires ===\n",
    "def vider_dossier(dossier):\n",
    "    if os.path.exists(dossier):\n",
    "        for f in os.listdir(dossier):\n",
    "            chemin = os.path.join(dossier, f)\n",
    "            if os.path.isfile(chemin):\n",
    "                os.remove(chemin)\n",
    "    else:\n",
    "        os.makedirs(dossier)\n",
    "\n",
    "# === 0. Définition des chemins ===\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "print(BASE_DIR)\n",
    "video_path = os.path.join(BASE_DIR, 'Result/videos/Lettres/I.mp4')\n",
    "extracted_dir = os.path.join(BASE_DIR, 'images_extraites')\n",
    "finger_dir = os.path.join(BASE_DIR, 'finger_find')\n",
    "frame_interval = 2\n",
    "\n",
    "# === 1. Nettoyage des dossiers ===\n",
    "vider_dossier(extracted_dir)\n",
    "vider_dossier(finger_dir)\n",
    "\n",
    "# === 2. Extraction des frames ===\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "if not cap.isOpened():\n",
    "    print(f\"Erreur : impossible d'ouvrir la vidéo '{video_path}'\")\n",
    "    exit()\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print(f\"Vidéo chargée : {total_frames} frames à {fps:.2f} fps\")\n",
    "\n",
    "frame_count = 0\n",
    "saved_count = 0\n",
    "\n",
    "while True:\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    if frame_count % frame_interval == 0:\n",
    "        filename = os.path.join(extracted_dir, f\"frame_{saved_count:04d}.jpg\")\n",
    "        cv2.imwrite(filename, frame)\n",
    "        saved_count += 1\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "cap.release()\n",
    "print(f\"{saved_count} images extraites dans le dossier '{extracted_dir}'\")\n",
    "\n",
    "# === 3. Détection du bout de l'index ===\n",
    "trace_points = []\n",
    "img_shape = None\n",
    "detector = HandDetector(staticMode=True, maxHands=1, detectionCon=0.7)\n",
    "\n",
    "#for filename in os.listdir(extracted_dir):\n",
    "for filename in sorted(os.listdir(extracted_dir)):\n",
    "    if not filename.endswith(('.jpg', '.png')):\n",
    "        continue\n",
    "\n",
    "    image_path = os.path.join(extracted_dir, filename)\n",
    "    image = cv2.imread(image_path)\n",
    "    hands, img = detector.findHands(image)\n",
    "\n",
    "    if hands:\n",
    "        hand = hands[0]\n",
    "        lm_list = hand['lmList']\n",
    "        if len(lm_list) >= 9:\n",
    "            x, y = lm_list[8][0], lm_list[8][1]\n",
    "            trace_points.append((x, y))\n",
    "\n",
    "    #cv2.imwrite(os.path.join(finger_dir, filename), img)\n",
    "\n",
    "# === 4. Génération de l'image composite ===\n",
    "#sample_img = cv2.imread(os.path.join(finger_dir, os.listdir(finger_dir)[0]))\n",
    "if img_shape is None:\n",
    "    img_shape = image.shape\n",
    "\n",
    "height, width, _ = img_shape\n",
    "result = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "\n",
    "for i in range(1, len(trace_points)):\n",
    "    cv2.line(result, trace_points[i - 1], trace_points[i], (0, 0, 255), thickness=6)\n",
    "\n",
    "# === 5. Rotation de 90° vers la droite ===\n",
    "rotated = cv2.rotate(result, cv2.ROTATE_90_CLOCKWISE)\n",
    "\n",
    "# === 6. Effet miroir (symétrie horizontale) ===\n",
    "mirrored = cv2.flip(rotated, 1)\n",
    "\n",
    "# === 7. Sauvegarde de l'image finale ===\n",
    "cv2.imwrite(\"../image_resultat.png\", mirrored)\n",
    "print(\"Image finale enregistrée sous 'image_resultat.png' (rotation + effet miroir)\")\n"
   ],
   "id": "f2c980429b41d943"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Traitements d'image",
   "id": "2cefe262d120695d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def save_step_logs(image, name, output_dir=\"debug_steps\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    if image is None or image.size == 0:\n",
    "        print(f\"[WARNING] Cannot save '{name}': image is empty.\")\n",
    "        return\n",
    "    cv2.imwrite(os.path.join(output_dir, f\"{name}.png\"), image)\n",
    "\n",
    "# Step 1: Extract red from image\n",
    "def extract_red_mask(img):\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    lower_red1 = np.array([0, 70, 50])\n",
    "    upper_red1 = np.array([10, 255, 255])\n",
    "    lower_red2 = np.array([170, 70, 50])\n",
    "    upper_red2 = np.array([180, 255, 255])\n",
    "    mask1 = cv2.inRange(hsv, lower_red1, upper_red1)\n",
    "    mask2 = cv2.inRange(hsv, lower_red2, upper_red2)\n",
    "    return cv2.bitwise_or(mask1, mask2)\n",
    "\n",
    "# Step 2: Basic cleaning (open/close)\n",
    "def clean_mask(mask):\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "    return cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "# Step 3: Keep relevant parts\n",
    "def filter_components(mask, min_area=50):\n",
    "    labeled = label(mask)\n",
    "    cleaned = np.zeros_like(mask)\n",
    "    for region in regionprops(labeled):\n",
    "        if region.area >= min_area:\n",
    "            for y, x in region.coords:\n",
    "                cleaned[y, x] = 255\n",
    "    return cleaned\n",
    "\n",
    "# Step 4: Skeletonize\n",
    "def get_skeleton(mask):\n",
    "    return (skeletonize(mask > 0) * 255).astype(np.uint8)\n",
    "\n",
    "# Step 5 : Bold the ligne\n",
    "def thicken_mask(mask, size=3):\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (size, size))\n",
    "    return cv2.dilate(mask, kernel, iterations=1)\n",
    "\n",
    "# Step 6: Resize and center\n",
    "def center_and_resize(mask, output_size=28, margin=2):\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        return np.ones((output_size, output_size), dtype=np.uint8) * 255\n",
    "    x, y, w, h = cv2.boundingRect(max(contours, key=cv2.contourArea))\n",
    "    cropped = mask[y:y+h, x:x+w]\n",
    "    resized = cv2.resize(cropped, (output_size - 2 * margin, output_size - 2 * margin))\n",
    "    canvas = np.ones((output_size, output_size), dtype=np.uint8) * 255\n",
    "    cx = (output_size - resized.shape[1]) // 2\n",
    "    cy = (output_size - resized.shape[0]) // 2\n",
    "    canvas[cy:cy + resized.shape[0], cx:cx + resized.shape[1]] = 255 - resized\n",
    "\n",
    "    return canvas\n",
    "\n",
    "# Main function\n",
    "def process_image(path, output_dir=\"debug_steps\"):\n",
    "    img = cv2.imread(path)\n",
    "    save_step_logs(img, \"00_original\", output_dir)\n",
    "\n",
    "    mask = extract_red_mask(img)\n",
    "    save_step_logs(mask, \"01_red_mask\", output_dir)\n",
    "\n",
    "    cleaned = clean_mask(mask)\n",
    "    save_step_logs(cleaned, \"02_cleaned\", output_dir)\n",
    "\n",
    "    filtered = filter_components(cleaned)\n",
    "    save_step_logs(filtered, \"03_filtered\", output_dir)\n",
    "\n",
    "    skeleton = get_skeleton(filtered)\n",
    "    save_step_logs(skeleton, \"04_skeleton\", output_dir)\n",
    "\n",
    "    thickened = thicken_mask(skeleton, size=50)\n",
    "    save_step_logs(thickened, \"05_thickened\", output_dir)\n",
    "\n",
    "    final = center_and_resize(thickened)\n",
    "    save_step_logs(final, \"06_final\", output_dir)\n",
    "\n",
    "    print(\"[INFO] Simplified processing complete.\")\n",
    "    cv2.imwrite(os.path.join(\"../Resultats/Conversion\", \"result.png\"), final)\n",
    "\n",
    "    return final\n",
    "\n",
    "# Run on your image\n",
    "process_image(\"../image_resultat.png\")"
   ],
   "id": "72659cdcf5d10507"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Application du Meilleur Modèle entrainé sur l'image\n",
   "id": "ff9002c4ce9379d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# TO DO",
   "id": "5ed9b480ebd1317"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Conclusions et Décisions\n",
    "---\n",
    "TO DO"
   ],
   "id": "a2b32a9886215680"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
