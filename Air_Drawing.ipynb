{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T14:51:28.519465Z",
     "start_time": "2025-06-02T14:51:28.267423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Imports\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "import kagglehub\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.applications import MobileNetV2, ResNet50V2\n",
    "from keras import layers, models\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Handtracking and Image modification\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "from skimage.morphology import skeletonize\n",
    "from skimage.measure import label, regionprops"
   ],
   "id": "3f1d3e5737ea2352",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Axel\\Desktop\\Air_Drawing\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Projet - Air Drawing\n",
    "---\n",
    "EXPLICATION DE LA PROBLEMATIQUE"
   ],
   "id": "30bbd429f65f49f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Datsets\n",
    "---"
   ],
   "id": "db1b647ba468abcc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Autre Dataset\n",
    "Explication de pourquoi est ce qu'on ne la pas pris car moins bien etc\n",
    "Solution : EMNIST"
   ],
   "id": "d610d762842bb693"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### EMNIST\n",
    "Le jeu de données EMNIST (Extended MNIST) est un ensemble de caractères manuscrits dérivé de la base de données NIST Special Database 19. Il a été converti au format d’image 28x28 pixels, avec une structure de données qui correspond directement à celle du jeu de données MNIST. EMNIST étend MNIST en incluant non seulement des chiffres, mais aussi des lettres majuscules et minuscules manuscrites, offrant ainsi un ensemble plus riche pour les tâches de reconnaissance de caractères."
   ],
   "id": "43e3c57ff07e0f50"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T14:55:38.565538Z",
     "start_time": "2025-06-02T14:52:42.455990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Télécharger dans le dossier par défaut non modifiable (cache de kagglehub)\n",
    "dataset_path = kagglehub.dataset_download(\"crawford/emnist\")\n",
    "\n",
    "# Dossier cible\n",
    "custom_path = \"./Datasets/emnist_datasets\"\n",
    "os.makedirs(custom_path, exist_ok=True)\n",
    "\n",
    "# Parcourir tout ce qu’il y a dans dataset_path\n",
    "for item in os.listdir(dataset_path):\n",
    "    src = os.path.join(dataset_path, item)\n",
    "    dst = os.path.join(custom_path, item)\n",
    "    shutil.move(src, dst)"
   ],
   "id": "e12f690a32b43121",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming download from 375390208 bytes (954793256 bytes left)...\n",
      "Resuming download from https://www.kaggle.com/api/v1/datasets/download/crawford/emnist?dataset_version_number=3 (375390208/1330183464) bytes left.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.24G/1.24G [02:26<00:00, 6.53MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### EMNIST (By_Class)\n",
    "L'ensemble complet de la base de données EMNIST est disponible dans le sous-ensembles ByClass.\n",
    "Le jeu ByClass contient 62 classes distinctes : les 10 chiffres (0–9), les 26 lettres majuscules (A–Z) et les 26 lettres minuscules (a–z).\n",
    "La répartition est déséquilibrée : certaines classes ont beaucoup plus d’exemples que d’autres.\n",
    "La fréquence des lettres reflète à peu près leur fréquence d'usage dans la langue anglaise.\n",
    "\n",
    "__Taille des ensembles :__\n",
    "- Entraînement : 697 932 images\n",
    "- Test : 116 323 images\n",
    "- Total : 814 255 images\n",
    "\n",
    "__Classes :__\n",
    "- ByClass : 62 classes (déséquilibrées)"
   ],
   "id": "9933c1334e506add"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### EMNIST (Letters)\n",
    "EXPLICATION"
   ],
   "id": "aeacadfa6b5afe06"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Entrainement de lettres\n",
    "---"
   ],
   "id": "874f068e71321ccb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Option A (CNN Maison)",
   "id": "f2ccc68d2194a28f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### CCN Maison 1",
   "id": "4c469c4ae2b0c248"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Modèle",
   "id": "43c4e66cd8b6a40"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# TO DO",
   "id": "5199db07d7bd5ad1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Tableau d'apprentissage",
   "id": "959b97c77a4f09f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# TO DO",
   "id": "6b005241b7adeccf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### CCN Maison 2",
   "id": "85e88e7277fdb498"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Modèle",
   "id": "20c3665720a7bcb5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# TO DO",
   "id": "ecb4448a5de0e45f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Tableau d'apprentissage",
   "id": "465da765a2ece86a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# TO DO",
   "id": "8281780cbce591ef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Option B (Transfert Learning)",
   "id": "9fc6b59c4c968ade"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Transfer Learning 1 (MobileNetV2)",
   "id": "2abb7ec715bf4edb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Modèle Feature Extraction",
   "id": "5fdfa4a23d219d5a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T08:40:11.255907Z",
     "start_time": "2025-06-02T08:40:11.253365Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load EMNIST Letters dataset\n",
    "def load_emnist_letters(root=\"./data\", train=True):\n",
    "    return torchvision.datasets.EMNIST(\n",
    "        root=root,\n",
    "        split=\"letters\",\n",
    "        train=train,\n",
    "        download=False,\n",
    "        transform=torchvision.transforms.ToTensor()\n",
    "    )"
   ],
   "id": "84b9315a6b7eb381",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T16:52:23.082524Z",
     "start_time": "2025-05-31T16:52:23.077640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert EMNIST to numpy arrays\n",
    "def emnist_to_numpy(dataset, max_samples=None):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for i, (img, label) in enumerate(dataset):\n",
    "        if max_samples and i >= max_samples:\n",
    "            break\n",
    "        img_np = img.numpy().squeeze()\n",
    "        images.append(img_np)\n",
    "        labels.append(label - 1)  # EMNIST Letters labels start at 1\n",
    "    return np.array(images), np.array(labels)\n"
   ],
   "id": "bfd2fec8baad54c4",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T16:52:25.185166Z",
     "start_time": "2025-05-31T16:52:25.180344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Build the transfer learning model\n",
    "def build_transfer_model():\n",
    "    base_model = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
    "    base_model.trainable = False\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(26, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ],
   "id": "cf6237b16dd45289",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T16:52:27.376045Z",
     "start_time": "2025-05-31T16:52:27.368805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Batch the data\n",
    "def create_emnist_tf_dataset(images, labels, batch_size=32, shuffle=True):\n",
    "    def preprocess(x, y):\n",
    "        x = tf.stack([x, x, x], axis=-1)\n",
    "        x = tf.image.resize(x, [224, 224])\n",
    "        return x, y\n",
    "    ds = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(images))\n",
    "    ds = ds.map(preprocess).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds"
   ],
   "id": "47dd77765c0d13a0",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T17:05:31.816766Z",
     "start_time": "2025-05-31T16:52:29.380146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = load_emnist_letters(train=True)\n",
    "test_dataset = load_emnist_letters(train=False)\n",
    "train_images, train_labels = emnist_to_numpy(train_dataset, max_samples=10000)\n",
    "test_images, test_labels = emnist_to_numpy(test_dataset, max_samples=2000)\n",
    "train_ds = create_emnist_tf_dataset(train_images, train_labels)\n",
    "test_ds = create_emnist_tf_dataset(test_images, test_labels, shuffle=False)\n",
    "\n",
    "# Train the model\n",
    "model = build_transfer_model()\n",
    "history = model.fit(train_ds, validation_data=test_ds, epochs=5)"
   ],
   "id": "354df7c7bd7bbf12",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001B[1m313/313\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m165s\u001B[0m 521ms/step - accuracy: 0.3961 - loss: 2.1390 - val_accuracy: 0.6815 - val_loss: 1.0693\n",
      "Epoch 2/5\n",
      "\u001B[1m313/313\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m172s\u001B[0m 550ms/step - accuracy: 0.7319 - loss: 0.8827 - val_accuracy: 0.7825 - val_loss: 0.7432\n",
      "Epoch 3/5\n",
      "\u001B[1m313/313\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m151s\u001B[0m 483ms/step - accuracy: 0.7861 - loss: 0.6751 - val_accuracy: 0.8020 - val_loss: 0.6331\n",
      "Epoch 4/5\n",
      "\u001B[1m313/313\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m146s\u001B[0m 468ms/step - accuracy: 0.8142 - loss: 0.5741 - val_accuracy: 0.8405 - val_loss: 0.5339\n",
      "Epoch 5/5\n",
      "\u001B[1m313/313\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m147s\u001B[0m 469ms/step - accuracy: 0.8287 - loss: 0.5205 - val_accuracy: 0.8235 - val_loss: 0.5833\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Tableau d'apprentissage (Feature Extraction)",
   "id": "2f42ad0ead12c2a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# TO DO",
   "id": "8a43dae76236c921"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Modèle Fine-tuning",
   "id": "6deabe09200a933d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T17:22:04.812753Z",
     "start_time": "2025-05-31T17:13:04.252329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Unfreeze top layers of the base model\n",
    "base_model = model.layers[0]  # Access MobileNetV2 inside the Sequential\n",
    "base_model.trainable = True\n",
    "\n",
    "# 2. Freeze most layers, keep top N trainable\n",
    "for layer in base_model.layers[:-30]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# 3. Recompile with a very low learning rate\n",
    "from keras.optimizers import Adam\n",
    "model.compile(optimizer=Adam(learning_rate=1e-5),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 4. Continue training\n",
    "fine_tune_epochs = 3\n",
    "model.fit(train_ds, validation_data=test_ds, epochs=fine_tune_epochs)\n"
   ],
   "id": "32e36caa48b641ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001B[1m313/313\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m209s\u001B[0m 653ms/step - accuracy: 0.4825 - loss: 1.8804 - val_accuracy: 0.8160 - val_loss: 0.6565\n",
      "Epoch 2/3\n",
      "\u001B[1m313/313\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m170s\u001B[0m 542ms/step - accuracy: 0.7621 - loss: 0.7337 - val_accuracy: 0.8250 - val_loss: 0.6061\n",
      "Epoch 3/3\n",
      "\u001B[1m313/313\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 473ms/step - accuracy: 0.8169 - loss: 0.5560"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-31 19:22:04.528019: W tensorflow/core/framework/op_kernel.cc:1829] UNKNOWN: KeyError: 0\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/home/lizardman/Coding/Projet_Machine_Learning_Air_Drawing/venv312/lib/python3.12/site-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n",
      "    ret = func(*args)\n",
      "          ^^^^^^^^^^^\n",
      "\n",
      "  File \"/home/lizardman/Coding/Projet_Machine_Learning_Air_Drawing/venv312/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"/home/lizardman/Coding/Projet_Machine_Learning_Air_Drawing/venv312/lib/python3.12/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 290, in finalize_py_func\n",
      "    generator_state.iterator_completed(iterator_id)\n",
      "\n",
      "  File \"/home/lizardman/Coding/Projet_Machine_Learning_Air_Drawing/venv312/lib/python3.12/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 872, in iterator_completed\n",
      "    del self._iterators[self._normalize_id(iterator_id)]\n",
      "        ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "KeyError: 0\n",
      "\n",
      "\n",
      "2025-05-31 19:22:04.528431: W tensorflow/core/kernels/data/generator_dataset_op.cc:108] Error occurred when finalizing GeneratorDataset iterator: UNKNOWN: KeyError: 0\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/home/lizardman/Coding/Projet_Machine_Learning_Air_Drawing/venv312/lib/python3.12/site-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n",
      "    ret = func(*args)\n",
      "          ^^^^^^^^^^^\n",
      "\n",
      "  File \"/home/lizardman/Coding/Projet_Machine_Learning_Air_Drawing/venv312/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"/home/lizardman/Coding/Projet_Machine_Learning_Air_Drawing/venv312/lib/python3.12/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 290, in finalize_py_func\n",
      "    generator_state.iterator_completed(iterator_id)\n",
      "\n",
      "  File \"/home/lizardman/Coding/Projet_Machine_Learning_Air_Drawing/venv312/lib/python3.12/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 872, in iterator_completed\n",
      "    del self._iterators[self._normalize_id(iterator_id)]\n",
      "        ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "KeyError: 0\n",
      "\n",
      "\n",
      "\t [[{{node PyFunc}}]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[25]\u001B[39m\u001B[32m, line 17\u001B[39m\n\u001B[32m     15\u001B[39m \u001B[38;5;66;03m# 4. Continue training\u001B[39;00m\n\u001B[32m     16\u001B[39m fine_tune_epochs = \u001B[32m3\u001B[39m  \u001B[38;5;66;03m# you can increase if needed\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m17\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_ds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtest_ds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfine_tune_epochs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Coding/Projet_Machine_Learning_Air_Drawing/venv312/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001B[39m, in \u001B[36mfilter_traceback.<locals>.error_handler\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    115\u001B[39m filtered_tb = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    116\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m117\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    118\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    119\u001B[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Coding/Projet_Machine_Learning_Air_Drawing/venv312/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:401\u001B[39m, in \u001B[36mTensorFlowTrainer.fit\u001B[39m\u001B[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001B[39m\n\u001B[32m    390\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33m_eval_epoch_iterator\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    391\u001B[39m     \u001B[38;5;28mself\u001B[39m._eval_epoch_iterator = TFEpochIterator(\n\u001B[32m    392\u001B[39m         x=val_x,\n\u001B[32m    393\u001B[39m         y=val_y,\n\u001B[32m   (...)\u001B[39m\u001B[32m    399\u001B[39m         shuffle=\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m    400\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m401\u001B[39m val_logs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    402\u001B[39m \u001B[43m    \u001B[49m\u001B[43mx\u001B[49m\u001B[43m=\u001B[49m\u001B[43mval_x\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    403\u001B[39m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[43m=\u001B[49m\u001B[43mval_y\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    404\u001B[39m \u001B[43m    \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m=\u001B[49m\u001B[43mval_sample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    405\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mvalidation_batch_size\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    406\u001B[39m \u001B[43m    \u001B[49m\u001B[43msteps\u001B[49m\u001B[43m=\u001B[49m\u001B[43mvalidation_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    407\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    408\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    409\u001B[39m \u001B[43m    \u001B[49m\u001B[43m_use_cached_eval_dataset\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    410\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    411\u001B[39m val_logs = {\n\u001B[32m    412\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mval_\u001B[39m\u001B[33m\"\u001B[39m + name: val \u001B[38;5;28;01mfor\u001B[39;00m name, val \u001B[38;5;129;01min\u001B[39;00m val_logs.items()\n\u001B[32m    413\u001B[39m }\n\u001B[32m    414\u001B[39m epoch_logs.update(val_logs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Coding/Projet_Machine_Learning_Air_Drawing/venv312/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001B[39m, in \u001B[36mfilter_traceback.<locals>.error_handler\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    115\u001B[39m filtered_tb = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    116\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m117\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    118\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    119\u001B[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Coding/Projet_Machine_Learning_Air_Drawing/venv312/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:489\u001B[39m, in \u001B[36mTensorFlowTrainer.evaluate\u001B[39m\u001B[34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, return_dict, **kwargs)\u001B[39m\n\u001B[32m    487\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m step, iterator \u001B[38;5;129;01min\u001B[39;00m epoch_iterator:\n\u001B[32m    488\u001B[39m     callbacks.on_test_batch_begin(step)\n\u001B[32m--> \u001B[39m\u001B[32m489\u001B[39m     logs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtest_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    490\u001B[39m     callbacks.on_test_batch_end(step, logs)\n\u001B[32m    491\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.stop_evaluating:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Coding/Projet_Machine_Learning_Air_Drawing/venv312/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:220\u001B[39m, in \u001B[36mTensorFlowTrainer._make_function.<locals>.function\u001B[39m\u001B[34m(iterator)\u001B[39m\n\u001B[32m    216\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mfunction\u001B[39m(iterator):\n\u001B[32m    217\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\n\u001B[32m    218\u001B[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001B[32m    219\u001B[39m     ):\n\u001B[32m--> \u001B[39m\u001B[32m220\u001B[39m         opt_outputs = \u001B[43mmulti_step_on_iterator\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    221\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m opt_outputs.has_value():\n\u001B[32m    222\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Coding/Projet_Machine_Learning_Air_Drawing/venv312/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001B[39m, in \u001B[36mfilter_traceback.<locals>.error_handler\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    148\u001B[39m filtered_tb = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    149\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m150\u001B[39m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    151\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    152\u001B[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Coding/Projet_Machine_Learning_Air_Drawing/venv312/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001B[39m, in \u001B[36mFunction.__call__\u001B[39m\u001B[34m(self, *args, **kwds)\u001B[39m\n\u001B[32m    830\u001B[39m compiler = \u001B[33m\"\u001B[39m\u001B[33mxla\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mnonXla\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    832\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m._jit_compile):\n\u001B[32m--> \u001B[39m\u001B[32m833\u001B[39m   result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    835\u001B[39m new_tracing_count = \u001B[38;5;28mself\u001B[39m.experimental_get_tracing_count()\n\u001B[32m    836\u001B[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Coding/Projet_Machine_Learning_Air_Drawing/venv312/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001B[39m, in \u001B[36mFunction._call\u001B[39m\u001B[34m(self, *args, **kwds)\u001B[39m\n\u001B[32m    875\u001B[39m \u001B[38;5;28mself\u001B[39m._lock.release()\n\u001B[32m    876\u001B[39m \u001B[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001B[39;00m\n\u001B[32m    877\u001B[39m \u001B[38;5;66;03m# run the first trace but we should fail if variables are created.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m878\u001B[39m results = \u001B[43mtracing_compilation\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcall_function\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    879\u001B[39m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_variable_creation_config\u001B[49m\n\u001B[32m    880\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    881\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._created_variables:\n\u001B[32m    882\u001B[39m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mCreating variables on a non-first call to a function\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    883\u001B[39m                    \u001B[33m\"\u001B[39m\u001B[33m decorated with tf.function.\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Coding/Projet_Machine_Learning_Air_Drawing/venv312/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001B[39m, in \u001B[36mcall_function\u001B[39m\u001B[34m(args, kwargs, tracing_options)\u001B[39m\n\u001B[32m    137\u001B[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001B[32m    138\u001B[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001B[32m--> \u001B[39m\u001B[32m139\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunction\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# pylint: disable=protected-access\u001B[39;49;00m\n\u001B[32m    140\u001B[39m \u001B[43m    \u001B[49m\u001B[43mflat_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfunction\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcaptured_inputs\u001B[49m\n\u001B[32m    141\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Coding/Projet_Machine_Learning_Air_Drawing/venv312/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001B[39m, in \u001B[36mConcreteFunction._call_flat\u001B[39m\u001B[34m(self, tensor_inputs, captured_inputs)\u001B[39m\n\u001B[32m   1318\u001B[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001B[32m   1319\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001B[32m   1320\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[32m   1321\u001B[39m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1322\u001B[39m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_inference_function\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcall_preflattened\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1323\u001B[39m forward_backward = \u001B[38;5;28mself\u001B[39m._select_forward_and_backward_functions(\n\u001B[32m   1324\u001B[39m     args,\n\u001B[32m   1325\u001B[39m     possible_gradient_type,\n\u001B[32m   1326\u001B[39m     executing_eagerly)\n\u001B[32m   1327\u001B[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Coding/Projet_Machine_Learning_Air_Drawing/venv312/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001B[39m, in \u001B[36mAtomicFunction.call_preflattened\u001B[39m\u001B[34m(self, args)\u001B[39m\n\u001B[32m    214\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcall_preflattened\u001B[39m(\u001B[38;5;28mself\u001B[39m, args: Sequence[core.Tensor]) -> Any:\n\u001B[32m    215\u001B[39m \u001B[38;5;250m  \u001B[39m\u001B[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m216\u001B[39m   flat_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcall_flat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    217\u001B[39m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.function_type.pack_output(flat_outputs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Coding/Projet_Machine_Learning_Air_Drawing/venv312/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001B[39m, in \u001B[36mAtomicFunction.call_flat\u001B[39m\u001B[34m(self, *args)\u001B[39m\n\u001B[32m    249\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m record.stop_recording():\n\u001B[32m    250\u001B[39m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._bound_context.executing_eagerly():\n\u001B[32m--> \u001B[39m\u001B[32m251\u001B[39m     outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_bound_context\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcall_function\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    252\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    253\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    254\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfunction_type\u001B[49m\u001B[43m.\u001B[49m\u001B[43mflat_outputs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    255\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    256\u001B[39m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    257\u001B[39m     outputs = make_call_op_in_graph(\n\u001B[32m    258\u001B[39m         \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    259\u001B[39m         \u001B[38;5;28mlist\u001B[39m(args),\n\u001B[32m    260\u001B[39m         \u001B[38;5;28mself\u001B[39m._bound_context.function_call_options.as_attrs(),\n\u001B[32m    261\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Coding/Projet_Machine_Learning_Air_Drawing/venv312/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1683\u001B[39m, in \u001B[36mContext.call_function\u001B[39m\u001B[34m(self, name, tensor_inputs, num_outputs)\u001B[39m\n\u001B[32m   1681\u001B[39m cancellation_context = cancellation.context()\n\u001B[32m   1682\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m cancellation_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1683\u001B[39m   outputs = \u001B[43mexecute\u001B[49m\u001B[43m.\u001B[49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1684\u001B[39m \u001B[43m      \u001B[49m\u001B[43mname\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mutf-8\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1685\u001B[39m \u001B[43m      \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1686\u001B[39m \u001B[43m      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtensor_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1687\u001B[39m \u001B[43m      \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1688\u001B[39m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   1689\u001B[39m \u001B[43m  \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1690\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1691\u001B[39m   outputs = execute.execute_with_cancellation(\n\u001B[32m   1692\u001B[39m       name.decode(\u001B[33m\"\u001B[39m\u001B[33mutf-8\u001B[39m\u001B[33m\"\u001B[39m),\n\u001B[32m   1693\u001B[39m       num_outputs=num_outputs,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1697\u001B[39m       cancellation_manager=cancellation_context,\n\u001B[32m   1698\u001B[39m   )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Coding/Projet_Machine_Learning_Air_Drawing/venv312/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001B[39m, in \u001B[36mquick_execute\u001B[39m\u001B[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[39m\n\u001B[32m     51\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m     52\u001B[39m   ctx.ensure_initialized()\n\u001B[32m---> \u001B[39m\u001B[32m53\u001B[39m   tensors = \u001B[43mpywrap_tfe\u001B[49m\u001B[43m.\u001B[49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     54\u001B[39m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     55\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m core._NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m     56\u001B[39m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Tableau d'apprentissage (Fine-tuning)",
   "id": "747ee788ea63db6c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# TO DO",
   "id": "dab9dbbdf61ab4d6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Transfer Learning 2 (ResNet50V2)",
   "id": "f84ad8cfd937bb9a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Modèle Feature Extraction",
   "id": "cbe0bdc945bac6af"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T08:47:52.133575Z",
     "start_time": "2025-06-02T08:47:52.130699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load EMNIST Letters dataset\n",
    "def load_emnist_letters(root=\"./data\", train=True):\n",
    "    return torchvision.datasets.EMNIST(\n",
    "        root=root,\n",
    "        split=\"letters\",\n",
    "        train=train,\n",
    "        download=False,\n",
    "        transform=torchvision.transforms.ToTensor()\n",
    "    )"
   ],
   "id": "6ead5f11a093667a",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T08:47:54.777462Z",
     "start_time": "2025-06-02T08:47:54.771198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert EMNIST to numpy arrays\n",
    "def emnist_to_numpy(dataset, max_samples=None):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for i, (img, label) in enumerate(dataset):\n",
    "        if max_samples and i >= max_samples:\n",
    "            break\n",
    "        img_np = img.numpy().squeeze()\n",
    "        images.append(img_np)\n",
    "        labels.append(label - 1)  # EMNIST Letters labels start at 1\n",
    "    return np.array(images), np.array(labels)"
   ],
   "id": "dd25f55bfd65eca",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Build the transfer learning model\n",
    "def build_transfer_model():\n",
    "    base_model = ResNet50V2(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False  # Freeze base model for feature extraction\n",
    "\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ],
   "id": "a322904526e62bbc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T08:47:58.168531Z",
     "start_time": "2025-06-02T08:47:58.161777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_transfer_model():\n",
    "    base_model = ResNet50V2(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
    "    base_model.trainable = False\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(26, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ],
   "id": "af40a2903e20a0fe",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T08:48:01.504349Z",
     "start_time": "2025-06-02T08:48:01.496312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Batch the data\n",
    "def create_emnist_tf_dataset(images, labels, batch_size=32, shuffle=True):\n",
    "    def preprocess(x, y):\n",
    "        x = tf.stack([x, x, x], axis=-1)\n",
    "        x = tf.image.resize(x, [224, 224])\n",
    "        return x, y\n",
    "    ds = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(images))\n",
    "    ds = ds.map(preprocess).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds"
   ],
   "id": "e3900e74cda30697",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T09:22:10.737085Z",
     "start_time": "2025-06-02T08:48:09.314674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = load_emnist_letters(train=True)\n",
    "test_dataset = load_emnist_letters(train=False)\n",
    "train_images, train_labels = emnist_to_numpy(train_dataset, max_samples=10000)\n",
    "test_images, test_labels = emnist_to_numpy(test_dataset, max_samples=2000)\n",
    "train_ds = create_emnist_tf_dataset(train_images, train_labels)\n",
    "test_ds = create_emnist_tf_dataset(test_images, test_labels, shuffle=False)\n",
    "\n",
    "# Train the model\n",
    "model = build_transfer_model()\n",
    "history = model.fit(train_ds, validation_data=test_ds, epochs=5)"
   ],
   "id": "fe40abe64fef48e7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-02 10:48:09.722188: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001B[1m94668760/94668760\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 0us/step\n",
      "Epoch 1/5\n",
      "\u001B[1m313/313\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m411s\u001B[0m 1s/step - accuracy: 0.4048 - loss: 2.0781 - val_accuracy: 0.8180 - val_loss: 0.7326\n",
      "Epoch 2/5\n",
      "\u001B[1m313/313\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m403s\u001B[0m 1s/step - accuracy: 0.7332 - loss: 0.8754 - val_accuracy: 0.8605 - val_loss: 0.5165\n",
      "Epoch 3/5\n",
      "\u001B[1m313/313\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m405s\u001B[0m 1s/step - accuracy: 0.7973 - loss: 0.6540 - val_accuracy: 0.8565 - val_loss: 0.5230\n",
      "Epoch 4/5\n",
      "\u001B[1m313/313\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m407s\u001B[0m 1s/step - accuracy: 0.8299 - loss: 0.5321 - val_accuracy: 0.8395 - val_loss: 0.5296\n",
      "Epoch 5/5\n",
      "\u001B[1m313/313\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m407s\u001B[0m 1s/step - accuracy: 0.8486 - loss: 0.4577 - val_accuracy: 0.8510 - val_loss: 0.5006\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Tableau d'apprentissage (Feature Extraction)",
   "id": "fbe069ed6bb82e1c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T09:58:59.479866Z",
     "start_time": "2025-06-02T09:30:55.195872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Unfreeze top layers of the base model\n",
    "base_model = model.layers[0]  # Access MobileNetV2 inside the Sequential\n",
    "base_model.trainable = True\n",
    "\n",
    "# 2. Freeze most layers, keep top N trainable\n",
    "for layer in base_model.layers[:-30]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# 3. Recompile with a very low learning rate\n",
    "from keras.optimizers import Adam\n",
    "model.compile(optimizer=Adam(learning_rate=1e-5),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 4. Continue training\n",
    "fine_tune_epochs = 3\n",
    "model.fit(train_ds, validation_data=test_ds, epochs=fine_tune_epochs)"
   ],
   "id": "5fa2072b56ce6d60",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001B[1m313/313\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m562s\u001B[0m 2s/step - accuracy: 0.6682 - loss: 1.0963 - val_accuracy: 0.8600 - val_loss: 0.4901\n",
      "Epoch 2/3\n",
      "\u001B[1m313/313\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m559s\u001B[0m 2s/step - accuracy: 0.8768 - loss: 0.3808 - val_accuracy: 0.8830 - val_loss: 0.4257\n",
      "Epoch 3/3\n",
      "\u001B[1m313/313\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m563s\u001B[0m 2s/step - accuracy: 0.9236 - loss: 0.2375 - val_accuracy: 0.8940 - val_loss: 0.4086\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7fa9ee5b4140>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 0. Unfreeze top layers of the base model\n",
    "base_model = model.layers[-1]  # Access MobileNetV2 inside the Sequential\n",
    "base_model.trainable = True\n",
    "\n",
    "# 1. Freeze most layers, keep top N trainable\n",
    "for layer in base_model.layers[:-31]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# 2. Recompile with a very low learning rate\n",
    "from keras.optimizers import Adam\n",
    "model.compile(optimizer=Adam(learning_rate=0e-5),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 3. Continue training\n",
    "fine_tune_epochs = 2\n",
    "model.fit(train_ds, validation_data=test_ds, epochs=fine_tune_epochs)"
   ],
   "id": "a56d96c809a8d612"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7c896ffec7a6f6cd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Modèle Fine-tuning",
   "id": "7d4dc07d7bbbfaf8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T10:28:03.133923Z",
     "start_time": "2025-06-02T09:59:57.390203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Unfreeze top layers of the base model\n",
    "base_model = model.layers[0]  # Access MobileNetV2 inside the Sequential\n",
    "base_model.trainable = True\n",
    "\n",
    "# 2. Freeze most layers, keep top N trainable\n",
    "for layer in base_model.layers[:-30]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# 3. Recompile with a very low learning rate\n",
    "from keras.optimizers import Adam\n",
    "model.compile(optimizer=Adam(learning_rate=1e-5),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 4. Continue training\n",
    "fine_tune_epochs = 3\n",
    "model.fit(train_ds, validation_data=test_ds, epochs=fine_tune_epochs)"
   ],
   "id": "afd8abf65aa561e0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001B[1m313/313\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m565s\u001B[0m 2s/step - accuracy: 0.9495 - loss: 0.1687 - val_accuracy: 0.8910 - val_loss: 0.3999\n",
      "Epoch 2/3\n",
      "\u001B[1m313/313\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m560s\u001B[0m 2s/step - accuracy: 0.9684 - loss: 0.1074 - val_accuracy: 0.8975 - val_loss: 0.3987\n",
      "Epoch 3/3\n",
      "\u001B[1m313/313\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m561s\u001B[0m 2s/step - accuracy: 0.9779 - loss: 0.0746 - val_accuracy: 0.9095 - val_loss: 0.3679\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7fa952a14f20>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Tableau d'apprentissage (Fine-tuning)",
   "id": "bae40c2c183b3b1d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# TO DO",
   "id": "7d9c89e01d41167f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Évaluation Comparative et Analyse Critique\n",
    "---\n",
    "Explication et résumer des résultat avec les tableaux etc\n",
    "Meilleur modele dans quel cas et pourquoi (temps, MSE, accuracy, etc)"
   ],
   "id": "5d7c813d2b5ce0ae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Application réel du projet\n",
    "---"
   ],
   "id": "19d602244aceca93"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Vidéo\n",
   "id": "c64cf9968ff01548"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === Fonctions utilitaires ===\n",
    "def vider_dossier(dossier):\n",
    "    if os.path.exists(dossier):\n",
    "        for f in os.listdir(dossier):\n",
    "            chemin = os.path.join(dossier, f)\n",
    "            if os.path.isfile(chemin):\n",
    "                os.remove(chemin)\n",
    "    else:\n",
    "        os.makedirs(dossier)\n",
    "\n",
    "# === 0. Définition des chemins ===\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "print(BASE_DIR)\n",
    "video_path = os.path.join(BASE_DIR, 'Result/videos/Lettres/I.mp4')\n",
    "extracted_dir = os.path.join(BASE_DIR, 'images_extraites')\n",
    "finger_dir = os.path.join(BASE_DIR, 'finger_find')\n",
    "frame_interval = 2\n",
    "\n",
    "# === 1. Nettoyage des dossiers ===\n",
    "vider_dossier(extracted_dir)\n",
    "vider_dossier(finger_dir)\n",
    "\n",
    "# === 2. Extraction des frames ===\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "if not cap.isOpened():\n",
    "    print(f\"Erreur : impossible d'ouvrir la vidéo '{video_path}'\")\n",
    "    exit()\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print(f\"Vidéo chargée : {total_frames} frames à {fps:.2f} fps\")\n",
    "\n",
    "frame_count = 0\n",
    "saved_count = 0\n",
    "\n",
    "while True:\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    if frame_count % frame_interval == 0:\n",
    "        filename = os.path.join(extracted_dir, f\"frame_{saved_count:04d}.jpg\")\n",
    "        cv2.imwrite(filename, frame)\n",
    "        saved_count += 1\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "cap.release()\n",
    "print(f\"{saved_count} images extraites dans le dossier '{extracted_dir}'\")\n",
    "\n",
    "# === 3. Détection du bout de l'index ===\n",
    "trace_points = []\n",
    "img_shape = None\n",
    "detector = HandDetector(staticMode=True, maxHands=1, detectionCon=0.7)\n",
    "\n",
    "#for filename in os.listdir(extracted_dir):\n",
    "for filename in sorted(os.listdir(extracted_dir)):\n",
    "    if not filename.endswith(('.jpg', '.png')):\n",
    "        continue\n",
    "\n",
    "    image_path = os.path.join(extracted_dir, filename)\n",
    "    image = cv2.imread(image_path)\n",
    "    hands, img = detector.findHands(image)\n",
    "\n",
    "    if hands:\n",
    "        hand = hands[0]\n",
    "        lm_list = hand['lmList']\n",
    "        if len(lm_list) >= 9:\n",
    "            x, y = lm_list[8][0], lm_list[8][1]\n",
    "            trace_points.append((x, y))\n",
    "\n",
    "    #cv2.imwrite(os.path.join(finger_dir, filename), img)\n",
    "\n",
    "# === 4. Génération de l'image composite ===\n",
    "#sample_img = cv2.imread(os.path.join(finger_dir, os.listdir(finger_dir)[0]))\n",
    "if img_shape is None:\n",
    "    img_shape = image.shape\n",
    "\n",
    "height, width, _ = img_shape\n",
    "result = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "\n",
    "for i in range(1, len(trace_points)):\n",
    "    cv2.line(result, trace_points[i - 1], trace_points[i], (0, 0, 255), thickness=6)\n",
    "\n",
    "# === 5. Rotation de 90° vers la droite ===\n",
    "rotated = cv2.rotate(result, cv2.ROTATE_90_CLOCKWISE)\n",
    "\n",
    "# === 6. Effet miroir (symétrie horizontale) ===\n",
    "mirrored = cv2.flip(rotated, 1)\n",
    "\n",
    "# === 7. Sauvegarde de l'image finale ===\n",
    "cv2.imwrite(\"../image_resultat.png\", mirrored)\n",
    "print(\"Image finale enregistrée sous 'image_resultat.png' (rotation + effet miroir)\")\n"
   ],
   "id": "90f763730f7905e6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Traitements d'image",
   "id": "aae0c493bf1d7865"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def save_step_logs(image, name, output_dir=\"debug_steps\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    if image is None or image.size == 0:\n",
    "        print(f\"[WARNING] Cannot save '{name}': image is empty.\")\n",
    "        return\n",
    "    cv2.imwrite(os.path.join(output_dir, f\"{name}.png\"), image)\n",
    "\n",
    "# Step 1: Extract red from image\n",
    "def extract_red_mask(img):\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    lower_red1 = np.array([0, 70, 50])\n",
    "    upper_red1 = np.array([10, 255, 255])\n",
    "    lower_red2 = np.array([170, 70, 50])\n",
    "    upper_red2 = np.array([180, 255, 255])\n",
    "    mask1 = cv2.inRange(hsv, lower_red1, upper_red1)\n",
    "    mask2 = cv2.inRange(hsv, lower_red2, upper_red2)\n",
    "    return cv2.bitwise_or(mask1, mask2)\n",
    "\n",
    "# Step 2: Basic cleaning (open/close)\n",
    "def clean_mask(mask):\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "    return cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "# Step 3: Keep relevant parts\n",
    "def filter_components(mask, min_area=50):\n",
    "    labeled = label(mask)\n",
    "    cleaned = np.zeros_like(mask)\n",
    "    for region in regionprops(labeled):\n",
    "        if region.area >= min_area:\n",
    "            for y, x in region.coords:\n",
    "                cleaned[y, x] = 255\n",
    "    return cleaned\n",
    "\n",
    "# Step 4: Skeletonize\n",
    "def get_skeleton(mask):\n",
    "    return (skeletonize(mask > 0) * 255).astype(np.uint8)\n",
    "\n",
    "# Step 5 : Bold the ligne\n",
    "def thicken_mask(mask, size=3):\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (size, size))\n",
    "    return cv2.dilate(mask, kernel, iterations=1)\n",
    "\n",
    "# Step 6: Resize and center\n",
    "def center_and_resize(mask, output_size=28, margin=2):\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        return np.ones((output_size, output_size), dtype=np.uint8) * 255\n",
    "    x, y, w, h = cv2.boundingRect(max(contours, key=cv2.contourArea))\n",
    "    cropped = mask[y:y+h, x:x+w]\n",
    "    resized = cv2.resize(cropped, (output_size - 2 * margin, output_size - 2 * margin))\n",
    "    canvas = np.ones((output_size, output_size), dtype=np.uint8) * 255\n",
    "    cx = (output_size - resized.shape[1]) // 2\n",
    "    cy = (output_size - resized.shape[0]) // 2\n",
    "    canvas[cy:cy + resized.shape[0], cx:cx + resized.shape[1]] = 255 - resized\n",
    "\n",
    "    return canvas\n",
    "\n",
    "# Main function\n",
    "def process_image(path, output_dir=\"debug_steps\"):\n",
    "    img = cv2.imread(path)\n",
    "    save_step_logs(img, \"00_original\", output_dir)\n",
    "\n",
    "    mask = extract_red_mask(img)\n",
    "    save_step_logs(mask, \"01_red_mask\", output_dir)\n",
    "\n",
    "    cleaned = clean_mask(mask)\n",
    "    save_step_logs(cleaned, \"02_cleaned\", output_dir)\n",
    "\n",
    "    filtered = filter_components(cleaned)\n",
    "    save_step_logs(filtered, \"03_filtered\", output_dir)\n",
    "\n",
    "    skeleton = get_skeleton(filtered)\n",
    "    save_step_logs(skeleton, \"04_skeleton\", output_dir)\n",
    "\n",
    "    thickened = thicken_mask(skeleton, size=50)\n",
    "    save_step_logs(thickened, \"05_thickened\", output_dir)\n",
    "\n",
    "    final = center_and_resize(thickened)\n",
    "    save_step_logs(final, \"06_final\", output_dir)\n",
    "\n",
    "    print(\"[INFO] Simplified processing complete.\")\n",
    "    cv2.imwrite(os.path.join(\"../Resultats/Conversion\", \"result.png\"), final)\n",
    "\n",
    "    return final\n",
    "\n",
    "# Run on your image\n",
    "process_image(\"../image_resultat.png\")"
   ],
   "id": "c64c1ca6347516f8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Application du Meilleur Modèle entrainé sur l'image\n",
   "id": "d023b33e7d8a802a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# TO DO",
   "id": "91cc16adfc7ff593"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Conclusions et Décisions\n",
    "---\n",
    "TO DO"
   ],
   "id": "a468053764ea2bc3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
